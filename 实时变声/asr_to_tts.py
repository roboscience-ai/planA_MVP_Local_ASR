#!/usr/bin/env python3
"""
é˜¿é‡Œäº‘ASR â†’ ç«å±±å¼•æ“TTS å®æ—¶è¯­éŸ³å›å£°
- éº¦å…‹é£è¾“å…¥ â†’ ASRè¯†åˆ« â†’ è¯†åˆ«å®Œæˆçš„å¥å­ â†’ TTSåˆæˆæ’­æ”¾
- åªæ’­æ”¾å®Œæ•´å¥å­ï¼ˆsentence_end = Trueï¼‰
"""
import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '../è¯­éŸ³'))

import pyaudio
import asyncio
import json
import uuid
import websockets
from threading import Thread, Lock
from dashscope.audio.asr import Recognition, RecognitionCallback, RecognitionResult
import dashscope
from dashscope.common.error import InvalidParameter

# ä»ç«å±±å¼•æ“åè®®åº“å¯¼å…¥
from protocols import EventType, MsgType, full_client_request, receive_message

# ==================== é…ç½® ====================
# é˜¿é‡Œäº‘ DashScope API Key
DASHSCOPE_API_KEY = "sk-3bf1277c421648329ba41f0a4f7c9549"

# ç«å±±å¼•æ“TTSé…ç½®
VOLC_APP_ID = "2634661217"
VOLC_ACCESS_TOKEN = "0im2q3lyhxDTTt5GXNtzmNSj2-I_Lb3b"
VOLC_VOICE_TYPE = "zh_male_naiqimengwa_mars_bigtts"  # ä¸»éŸ³è‰²ï¼ˆç”·å£°ï¼‰
VOLC_FEMALE_VOICE = "ICL_zh_female_bingruoshaonv_tob"  # å¥³å£°éŸ³è‰²ï¼ˆç”¨äºæ··åˆï¼‰
USE_MIXED_VOICE = True  # æ˜¯å¦ä½¿ç”¨æ··åˆéŸ³è‰²
MALE_MIX_FACTOR = 0.45  # ç”·å£°æ··åˆæ¯”ä¾‹ï¼ˆ65%ï¼‰
FEMALE_MIX_FACTOR = 0.55  # å¥³å£°æ··åˆæ¯”ä¾‹ï¼ˆ35%ï¼‰
TTS_ENDPOINT = "wss://openspeech.bytedance.com/api/v3/tts/unidirectional/stream"

# éŸ³é¢‘å‚æ•°
FORMAT = pyaudio.paInt16
CHANNELS = 1
RATE_ASR = 16000  # ASRè¾“å…¥
RATE_TTS = 24000  # TTSè¾“å‡º

# å¾…æ’­æ”¾çš„å¥å­é˜Ÿåˆ—
sentence_list = []
sentence_lock = Lock()
tts_running = True
# =================================================

def get_resource_id(use_mixed: bool = False) -> str:
    """æ ¹æ®æ˜¯å¦ä½¿ç”¨æ··åˆéŸ³è‰²é€‰æ‹©Resource ID"""
    # æ ¹æ®ç«å±±å¼•æ“æ–‡æ¡£ï¼Œæ··åˆéŸ³è‰²åº”ä½¿ç”¨ volc.service_type.10029
    if use_mixed:
        return "volc.service_type.10029"
    # å•ä¸€éŸ³è‰²æ—¶ï¼Œæ ¹æ®éŸ³è‰²ç±»å‹åˆ¤æ–­
    if VOLC_VOICE_TYPE.startswith("S_"):
        return "volc.megatts.default"
    return "volc.service_type.10029"

async def tts_synthesize(text: str) -> bytes:
    """
    ä½¿ç”¨ç«å±±å¼•æ“TTSåˆæˆè¯­éŸ³
    è¿”å›PCMéŸ³é¢‘æ•°æ®
    """
    headers = {
        "X-Api-App-Key": VOLC_APP_ID,
        "X-Api-Access-Key": VOLC_ACCESS_TOKEN,
        "X-Api-Resource-Id": get_resource_id(use_mixed=USE_MIXED_VOICE),
        "X-Api-Connect-Id": str(uuid.uuid4()),
    }

    try:
        websocket = await websockets.connect(
            TTS_ENDPOINT, 
            extra_headers=headers, 
            max_size=10 * 1024 * 1024
        )
        
        # å‡†å¤‡è¯·æ±‚å‚æ•°
        req_params = {
            "audio_params": {
                "format": "pcm",
                "sample_rate": RATE_TTS,
                "enable_timestamp": False,
            },
            "text": text,
            "additions": json.dumps({"disable_markdown_filter": False}),
        }
        
        # æ ¹æ®é…ç½®é€‰æ‹©ä½¿ç”¨å•ä¸€éŸ³è‰²è¿˜æ˜¯æ··åˆéŸ³è‰²
        if USE_MIXED_VOICE:
            # æ··åˆéŸ³è‰²ï¼šspeaker è®¾ç½®ä¸º custom_mix_bigttsï¼Œæ·»åŠ  mix_speaker å‚æ•°
            req_params["speaker"] = "custom_mix_bigtts"
            req_params["mix_speaker"] = {
                "speakers": [
                    {
                        "source_speaker": VOLC_VOICE_TYPE,  # ç”·å£°
                        "mix_factor": MALE_MIX_FACTOR
                    },
                    {
                        "source_speaker": VOLC_FEMALE_VOICE,  # å¥³å£°
                        "mix_factor": FEMALE_MIX_FACTOR
                    }
                ]
            }
        else:
            # å•ä¸€éŸ³è‰²ï¼šç›´æ¥ä½¿ç”¨ speaker å­—æ®µ
            req_params["speaker"] = VOLC_VOICE_TYPE
        
        request = {
            "user": {"uid": str(uuid.uuid4())},
            "req_params": req_params,
        }
        
        # å‘é€è¯·æ±‚
        await full_client_request(websocket, json.dumps(request).encode())
        
        # æ¥æ”¶éŸ³é¢‘æ•°æ®
        audio_data = bytearray()
        while True:
            msg = await receive_message(websocket)
            
            if msg.type == MsgType.FullServerResponse:
                if msg.event == EventType.SessionFinished:
                    break
            elif msg.type == MsgType.AudioOnlyServer:
                audio_data.extend(msg.payload)
            else:
                print(f"âš ï¸  TTSé”™è¯¯: {msg}")
                break
        
        await websocket.close()
        return bytes(audio_data)
        
    except Exception as e:
        print(f"âŒ TTSåˆæˆå¤±è´¥: {e}")
        return b""

def play_audio(audio_data: bytes):
    """æ’­æ”¾PCMéŸ³é¢‘æ•°æ®"""
    if not audio_data:
        return
    
    p = pyaudio.PyAudio()
    try:
        stream = p.open(
            format=FORMAT,
            channels=CHANNELS,
            rate=RATE_TTS,
            output=True,
            frames_per_buffer=1024
        )
        stream.write(audio_data)
        stream.stop_stream()
        stream.close()
    except Exception as e:
        print(f"âŒ æ’­æ”¾å¤±è´¥: {e}")
    finally:
        p.terminate()

# ==================== TTSå¤„ç†çº¿ç¨‹ ====================
def tts_worker():
    """TTSåˆæˆå’Œæ’­æ”¾çº¿ç¨‹"""
    global tts_running
    
    print("ğŸ”Š TTSæ’­æ”¾å™¨å·²å¯åŠ¨")
    print("ğŸ“‹ ç­‰å¾…ASRè¯†åˆ«å®Œæ•´å¥å­...\n")
    
    processed_count = 0  # å·²å¤„ç†çš„å¥å­æ•°é‡
    
    while tts_running:
        try:
            # æ£€æŸ¥listä¸­æ˜¯å¦æœ‰æ–°å¥å­
            text = None
            with sentence_lock:
                if len(sentence_list) > processed_count:
                    text = sentence_list[processed_count]
                    processed_count += 1
            
            if text:
                print(f"\nğŸ¯ [{processed_count}] å®Œæ•´å¥å­: {text}")
                print(f"ğŸ”„ æ­£åœ¨åˆæˆè¯­éŸ³...")
                
                # å¼‚æ­¥åˆæˆ
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                audio_data = loop.run_until_complete(tts_synthesize(text))
                loop.close()
                
                if audio_data:
                    print(f"â–¶ï¸  æ’­æ”¾ä¸­ ({len(audio_data)} å­—èŠ‚)...")
                    play_audio(audio_data)
                    print(f"âœ… æ’­æ”¾å®Œæˆ\n")
                else:
                    print(f"âŒ TTSåˆæˆå¤±è´¥\n")
            else:
                # æ²¡æœ‰æ–°å¥å­ï¼Œç­‰å¾…ä¸€ä¸‹
                import time
                time.sleep(0.1)
                        
        except Exception as e:
            if tts_running:
                print(f"âŒ TTSå¤„ç†é”™è¯¯: {e}")
    
    print("\nğŸ”Š TTSæ’­æ”¾å™¨å·²å…³é—­")
    print(f"ğŸ“Š å…±æ’­æ”¾ {processed_count} å¥è¯")

# ==================== é˜¿é‡Œäº‘ASR ====================
mic = None
stream = None
recognition_running = False

class Callback(RecognitionCallback):
    def on_open(self) -> None:
        global mic, stream, recognition_running
        print("âœ… é˜¿é‡Œäº‘ASRå·²å¯åŠ¨")
        print("ğŸ™ï¸ è¯·å¼€å§‹è¯´è¯ï¼ˆè¯†åˆ«åˆ°å®Œæ•´å¥å­ä¼šè‡ªåŠ¨æ’­æ”¾ï¼‰\n")
        recognition_running = True
        
        mic = pyaudio.PyAudio()
        
        # æ˜¾å¼æŒ‡å®šé»˜è®¤è¾“å…¥è®¾å¤‡
        try:
            default_input = mic.get_default_input_device_info()
            stream = mic.open(
                format=FORMAT, 
                channels=CHANNELS, 
                rate=RATE_ASR, 
                input=True,
                input_device_index=default_input['index'],
                frames_per_buffer=3200
            )
            print(f"ğŸ“± ä½¿ç”¨è¾“å…¥è®¾å¤‡: {default_input['name']}\n")
        except Exception as e:
            print(f"âŒ æ‰“å¼€éº¦å…‹é£å¤±è´¥: {e}")
            raise

    def on_close(self) -> None:
        global mic, stream, tts_running, recognition_running
        print("\nâœ… ASRè¯†åˆ«ç»“æŸ")
        recognition_running = False
        tts_running = False  # åœæ­¢TTSçº¿ç¨‹
        try:
            if stream is not None:
                stream.stop_stream()
                stream.close()
        except Exception:
            pass
        finally:
            stream = None
        try:
            if mic is not None:
                mic.terminate()
        except Exception:
            pass
        finally:
            mic = None

    def on_event(self, result: RecognitionResult) -> None:
        sentence = result.get_sentence()
        
        if sentence:
            # å®æ—¶æ˜¾ç¤ºè¯†åˆ«ä¸­çš„æ–‡æœ¬ï¼ˆsentence_end = Falseï¼‰
            if not sentence.get('sentence_end', False):
                text = sentence.get('text', '').strip()
                if text:
                    print(f"\rğŸ’¬ è¯†åˆ«ä¸­: {text}", end='', flush=True)
            
            # åªå¤„ç†å®Œæ•´å¥å­ï¼ˆsentence_end = Trueï¼‰
            elif sentence.get('sentence_end', True):
                text = sentence.get('text', '').strip()
                if text:
                    print(f"\râœ… å®Œæ•´å¥å­: {text}")
                    # æ·»åŠ åˆ°é˜Ÿåˆ—
                    with sentence_lock:
                        sentence_list.append(text)
                        print(f"ğŸ“ å·²åŠ å…¥æ’­æ”¾é˜Ÿåˆ— (å…±{len(sentence_list)}å¥)\n")

# ==================== ä¸»å‡½æ•° ====================
def main():
    global tts_running, recognition_running
    
    # è®¾ç½®é˜¿é‡Œäº‘API Key
    dashscope.api_key = DASHSCOPE_API_KEY
    
    print("=" * 60)
    print("ğŸ™ï¸  é˜¿é‡Œäº‘ASR â†’ ç«å±±å¼•æ“TTS å®æ—¶è¯­éŸ³å›å£°")
    print("=" * 60)
    print(f"ASR: é˜¿é‡Œäº‘ Paraformer V2 (å®æ—¶è¯†åˆ«)")
    if USE_MIXED_VOICE:
        print(f"TTS: æ··åˆéŸ³è‰² (æ›´å¥³æ€§åŒ–)")
        print(f"  - ä¸»éŸ³è‰²: {VOLC_VOICE_TYPE} ({MALE_MIX_FACTOR*100:.0f}%)")
        print(f"  - å¥³å£°éŸ³è‰²: {VOLC_FEMALE_VOICE} ({FEMALE_MIX_FACTOR*100:.0f}%)")
    else:
        print(f"TTS: ç«å±±å¼•æ“ {VOLC_VOICE_TYPE}")
    print(f"æ¨¡å¼: åªæ’­æ”¾å®Œæ•´å¥å­ (sentence_end = True)")
    print("=" * 60)
    print()
    
    # å±è”½websocketsçš„INFOæ—¥å¿—
    import logging
    logging.getLogger("websockets").setLevel(logging.WARNING)
    logging.getLogger("protocols.protocols").setLevel(logging.WARNING)
    
    # å¯åŠ¨TTSå¤„ç†çº¿ç¨‹
    tts_thread = Thread(target=tts_worker, daemon=True)
    tts_thread.start()
    
    # å¯åŠ¨ASRè¯†åˆ«
    recognition = Recognition(
        model="paraformer-realtime-v2",
        format="pcm",
        sample_rate=RATE_ASR,
        language_hints=["zh"],
        disfluency_removal_enabled=True,  # å»æ‰"å—¯"ã€"å•Š"
        callback=Callback()
    )
    
    recognition.start()
    
    # ç­‰å¾… stream åˆå§‹åŒ–
    import time
    timeout = 5  # ç­‰å¾…æœ€å¤š5ç§’
    start_time = time.time()
    while stream is None and (time.time() - start_time) < timeout:
        time.sleep(0.1)
    
    if stream is None:
        print("âŒ æ— æ³•åˆå§‹åŒ–éŸ³é¢‘æµ")
        recognition.stop()
        tts_running = False
        return
    
    try:
        # æŒç»­å‘é€éŸ³é¢‘
        while recognition_running and stream and tts_running:
            try:
                data = stream.read(3200, exception_on_overflow=False)
                recognition.send_audio_frame(data)
            except InvalidParameter:
                # è¯†åˆ«å·²åœæ­¢ï¼Œé€€å‡ºå‘é€å¾ªç¯
                break
            except Exception as e:
                if recognition_running:
                    print(f"âš ï¸  å‘é€éŸ³é¢‘å¸§é”™è¯¯: {e}")
                break
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸  ç”¨æˆ·ä¸­æ–­")
    finally:
        try:
            if recognition_running:
                recognition.stop()
        except (InvalidParameter, Exception):
            # å·²åœæ­¢æˆ–å‡ºé”™åˆ™å¿½ç•¥
            pass
        tts_running = False
        # ç­‰å¾…TTSçº¿ç¨‹å¤„ç†å®Œ
        print("\nâ³ ç­‰å¾…æ’­æ”¾é˜Ÿåˆ—æ¸…ç©º...")
        tts_thread.join(timeout=5)
        print("\nğŸ‘‹ ç¨‹åºé€€å‡º")

# ==================== å¯åŠ¨ ====================
if __name__ == "__main__":
    main()

